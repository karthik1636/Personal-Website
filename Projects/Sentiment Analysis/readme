
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Project Portfolio</title>
<link rel="stylesheet" href="https://path_to_your_icon_set.css">
<style>
    body { font-family: Arial, sans-serif; }
    .icon { padding-right: 5px; }
</style>
</head>
<body>
<section>
    <h1>Comparative Analysis of LSTM and Transformer Models for Emotion Detection in Text</h1>
    <p><strong>Objective:</strong> Evaluate the effectiveness of LSTM and Transformer models in detecting emotions from textual data to enhance applications in customer feedback, mental health, and market research.</p>
    
    <h2>Data Description</h2>
    <p><i class="icon general-icon-data-size"></i>Size: Over 839,000 annotated text entries.</p>
    <p><i class="icon general-icon-data-source"></i>Sources: Compiled from publicly available NLP datasets and supplemented by manual annotations for diversity.</p>
    
    <h2>Methodology</h2>
    <ol>
        <li><i class="icon general-icon-preprocess"></i>Data Preprocessing:
            <ul>
                <li>Tokenization: Mapping words to integers for structured model input.</li>
                <li>Padding: Uniform sequence length for consistent neural network input.</li>
            </ul>
        </li>
        <li><i class="icon general-icon-architecture"></i>Model Architecture:
            <ul>
                <li>LSTM: Gates-based model addressing vanishing gradients.</li>
                <li>Transformer: Encoder-decoder with multi-headed self-attention.</li>
            </ul>
        </li>
        <li><i class="icon general-icon-training"></i>Training:
            <ul>
                <li>GPU: RTX 4060.</li>
                <li>LSTM: 5 epochs, ~12 min each.</li>
                <li>Transformer: Total 14 min training.</li>
            </ul>
        </li>
    </ol>
    
    <h2>Performance Metrics</h2>
    <p><i class="icon general-icon-accuracy"></i>Accuracy:
        <ul>
            <li>LSTM: 99.2%</li>
            <li>Transformer: 99.8%</li>
        </ul>
    </p>
    <p><i class="icon general-icon-metrics"></i>Precision, Recall, F1-Score:
        <ul>
            <li>LSTM: Precision 99.2%, Recall 99.1%, F1 99.15%</li>
            <li>Transformer: Precision 99.8%, Recall 99.75%, F1 99.775%</li>
        </ul>
    </p>
    
    <h2>Quantifiable Results</h2>
    <p><i class="icon general-icon-efficiency"></i>Data Efficiency: Transformer models outperform LSTMs in accuracy and training speed.</p>
    
    <h2>Project Impact</h2>
    <p><i class="icon general-icon-academic"></i>Academic Contribution: Insights into emotion detection capabilities of LSTM and Transformer models.</p>
    <p><i class="icon general-icon-commercial"></i>Commercial Application: Enhanced accuracy for customer interactions and mental health monitoring.</p>
    <p><i class="icon general-icon-technical"></i>Technical Advancement: Sets benchmarks for future NLP research with large datasets.</p>
    
    <h2>Tools and Technologies Used</h2>
    <p><i class="icon general-icon-python"></i>Python.</p>
    <p><i class="icon general-icon-tensorflow"></i>TensorFlow for LSTM, <i class="icon general-icon-pytorch"></i>PyTorch for Transformer.</p>
    <p><i class="icon general-icon-hardware"></i>NVIDIA RTX 4060 GPU.</p>
</section>
</body>
</html>
